Файл с комментариями по выбранным решениям для задания 1.

1. Определите стратегию масштабирования и отказоустойчивости.
2. Проработайте конфигурацию развёртывания приложения в Kubernetes
3. Спланируйте балансировку нагрузки
4. Определите наиболее подходящую фейловер-стратегию
5. Определите конфигурацию базы данных
6. Определите, будете ли вы применять шардирование БД


Стратегия масштабирования.
Вертикальное масштабирование с увеличением ресурсов как по мне поможет в самом начале появления волн нагрузок,
поэтому я выбираю горизонтальное т.к. мы с помощью данного варианта масштабирования сможем параллельно обрабатывать
сразу несколько запросов от разных клиентов, тем самым увеличив пропускную способность нашего приложения.

Конфигурация развёртывания приложения в k8s.
4 независимых кластера в соответствии со своей геозоной (2 кластера для ЦФО, 1 в Сибири и 1 на ДВ)*
Это создаст стабильный для каждой геозоны кластер, отвечающий за свой участок и сбой в одном, не создаст проблем для 
пользователей в другой геозоне.
Также что касается deployment's настроим auto-scaling в зависимости от нагрузки на pod'ы приложения для горизонтального
масштабирования приложения.

_*необходим анализ рынка, вероятно можно будет изменить кол-во кластеров в зависимости от кол-ва клиентов._

Балансировка нагрузки.
GSLB - для балансировки трафика по геозонам. 
Ingress-контроллер для локальной балансировки.

Фейловер-стратегия.
т.к. у нас требование бизнеса доступность 999, то предлагаю стратегию типа геозоны + active-Warm StandBy 
(с применением GSLB на случай отказа одной из геозон мы просто будет направлять трафик из другого ЦОД'а)
Также нужно разработать DRP план на случай отказа/сбоя системы для сотрудников.

Конфигурация БД
т.к. мы имеем сложную распределенную систему по всей стране, то и БД у нас должна поддерживать выбор данного решения.
В основе мы используем классическое решение с СУБД PostgreSQL. А для того, чтобы данные были согласованы и консистентны 
мы используем следующие инструменты: кластер etcd, HAProxy, pgBackRest, Patroni и PMM.
При помощи данных инструментов мы сможем создать распределенную БД к которой мы сможем обращаться из любого кластера через HAProxy.
А данные при помощи потоковой репликации будут находится в каждом узле нашей БД. pgBackRest позволит создавать своевременно бэкапы для
надежного хранения информации. 

Была идея использовать шардирование (горизонтальное) т.к. у нас есть геозоны, но как по мне это усложнит систему и здесь
лучше бы использовать Mongo иди ClickHouse, а не Postgresql т.к. у них встроены свои классные механизмы для шардирования, когда
Postgresql будет более сложно настроить. В дальнейшем конечно можно рассмотреть и этот вариант, если на предварительном тестировании
репликация не оправдает ожидания по быстродействию.

